{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS361 Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these experiments, we endeavour to find the best ensemble learning algorithm for the Arrhythmia, Caesarian and Website-Phishing datasets respectively, out of the Decision Tree, Random Forest, Bagging and AdaBoost methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Task 2, I chose the GradientBoosting algorithm, and implemented it along with the other four prior algorithms mentioned on the following datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from autorank import autorank, plot_stats, create_report, latex_table\n",
    "from statistics import stdev\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Render matplotlib plots in the notebook\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a random state to ensure that out experiments are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Random State\n",
    "rs = 1234\n",
    "np.random.seed(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrhythmia Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began by splitting each dataset into their feature set (X) and target set (y). For all datasets, the target was the final column ‘class’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(452, 279) (452,)\n"
     ]
    }
   ],
   "source": [
    "#Load Arrhythmia Dataset\n",
    "df = pd.read_csv('arrhythmia.csv', na_values = ['?'] ) #Converts '?' values to NaN\n",
    "X = df.loc[:, 'age':'chV6_QRSTA']\n",
    "y = df.loc[:, 'class']\n",
    "\n",
    "X = np.nan_to_num(X) #Converts remaining NA values to '0'\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we distinguished the training and test set at an 80:20 split.  As the name suggests, training data teaches the model, whereas test data provides insight into how well the model generalises to unseen data. Splitting the data into 80% training and 20% testing means the model will assess more known cases and hopefully come to an accurate solution, when tested against the unseen data. This is only appropriate for large datasets that would have enough data to enable a training/test split and be able to learn general principles from the training set, which is luckily provided by the datasets required in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(361, 279) (91, 279)\n"
     ]
    }
   ],
   "source": [
    "#Distinguish Training and Test Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = rs)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the ensemble algorithms on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=10, random_state=1234)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensemble Algorithms\n",
    "\n",
    "#Straight Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state = rs)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(n_estimators = 10, random_state = rs)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#AdaBoost\n",
    "adb = AdaBoostClassifier(DecisionTreeClassifier(random_state = rs), n_estimators = 10, learning_rate=1)\n",
    "adb.fit(X_train, y_train)\n",
    "\n",
    "#Bagging\n",
    "bg = BaggingClassifier(DecisionTreeClassifier(random_state = rs), n_estimators = 10)\n",
    "bg.fit(X_train, y_train)\n",
    "\n",
    "#Gradient Boosting\n",
    "gb = GradientBoostingClassifier(random_state = rs, n_estimators = 10)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we derive the accuracy score of each algorithm for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on train set: 1.00\n",
      "Decision Tree Accuracy on test set: 0.60\n",
      "\n",
      "Random Forest Accuracy on train set: 0.99\n",
      "Random Forest Accuracy on test set: 0.59\n",
      "\n",
      "AdaBoost Accuracy on train set: 1.00\n",
      "AdaBoost Accuracy on test set: 0.63\n",
      "\n",
      "Bagging Accuracy on train set: 0.98\n",
      "Bagging Accuracy on test set: 0.71\n",
      "\n",
      "Gradient Boosting Accuracy on train set: 0.95\n",
      "Gradient Boosting on test set: 0.62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Scores\n",
    "\n",
    "#Standard Decision Tree Score\n",
    "dt_score = dt.score(X_train, y_train)\n",
    "print(f'Decision Tree Accuracy on train set: {dt_score:.2f}')\n",
    "\n",
    "dt_score = dt.score(X_test, y_test)\n",
    "print(f'Decision Tree Accuracy on test set: {dt_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Random Forest Score\n",
    "rf_score = rf.score(X_train, y_train)\n",
    "print(f'Random Forest Accuracy on train set: {rf_score:.2f}')\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print(f'Random Forest Accuracy on test set: {rf_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#AdaBoost Score\n",
    "adb_score = adb.score(X_train, y_train)\n",
    "print(f'AdaBoost Accuracy on train set: {adb_score:.2f}')\n",
    "\n",
    "adb_score = adb.score(X_test, y_test)\n",
    "print(f'AdaBoost Accuracy on test set: {adb_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Bagging Score\n",
    "bg_score = bg.score(X_train, y_train)\n",
    "print(f'Bagging Accuracy on train set: {bg_score:.2f}')\n",
    "\n",
    "bg_score = bg.score(X_test, y_test)\n",
    "print(f'Bagging Accuracy on test set: {bg_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Gradient Boosting\n",
    "gb_score = gb.score(X_train, y_train)\n",
    "print(f'Gradient Boosting Accuracy on train set: {gb_score:.2f}')\n",
    "\n",
    "gb_score = gb.score(X_test, y_test)\n",
    "print(f'Gradient Boosting on test set: {gb_score:.2f}')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we proceed to cross-validate. Cross-validation evaluates models on the unseen data in the test set, by resampling. In particular, cross-validation splits a given data sample at parameter k and proceeds to run iterations of the experiment on each unique group (it splits them again into training and test data, fits a model on the training, evaluates on the test set and retains the evaluation score - discarding the model). This summarises the skill of the model using the sample of model evaluation scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experiment, I chose parameter k=10 as it is a common tactic that has been found to generally result in a model skill estimate with low bias and a modest variance, thus resulting in a 10-fold cross validation for each classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean test accuracy: 0.961\n",
      "Decision Tree Standard Deviation: 0.022\n",
      "\n",
      "Random Forest Mean test accuracy: 0.973\n",
      "Random Forest Standard Deviation: 0.011\n",
      "\n",
      "AdaBoost Mean test accuracy: 0.933\n",
      "AdaBoost Standard Deviation: 0.006\n",
      "\n",
      "Bagging Mean test accuracy: 0.969\n",
      "Bagging Standard Deviation: 0.014\n",
      "\n",
      "Gradient Boosting Mean test accuracy: 0.948\n",
      "Gradient Boosting Standard Deviation: 0.007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "means = []\n",
    "sds = []\n",
    "\n",
    "#Decision Trees\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Tree Mean test accuracy: {np.mean(dt_scores):.3f}\")\n",
    "means += [dt_scores]\n",
    "\n",
    "std = stdev(dt_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(rf_scores)\n",
    "print(f\"Random Forest Mean test accuracy: {np.mean(rf_scores):.3f}\")\n",
    "means += [rf_scores]\n",
    "\n",
    "\n",
    "std = stdev(rf_scores)\n",
    "sds += [std]\n",
    "print(f\"Random Forest Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#AdaBoost\n",
    "adb_scores = cross_val_score(AdaBoostClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(adb_scores)\n",
    "print(f\"AdaBoost Mean test accuracy: {np.mean(adb_scores):.3f}\")\n",
    "means += [adb_scores]\n",
    "\n",
    "std = stdev(adb_scores)\n",
    "sds += [std]\n",
    "print(f\"AdaBoost Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Bagging\n",
    "bg_scores = cross_val_score(BaggingClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(bg_scores)\n",
    "print(f\"Bagging Mean test accuracy: {np.mean(bg_scores):.3f}\")\n",
    "means += [bg_scores]\n",
    "\n",
    "std = stdev(bg_scores)\n",
    "sds += [std]\n",
    "print(f\"Bagging Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Grdient Boosting\n",
    "gb_scores = cross_val_score(GradientBoostingClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(bg_scores)\n",
    "print(f\"Gradient Boosting Mean test accuracy: {np.mean(gb_scores):.3f}\")\n",
    "means += [gb_scores]\n",
    "\n",
    "std = stdev(gb_scores)\n",
    "sds += [std]\n",
    "print(f\"Gradient Boosting Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autorank Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I created a data frame comprised of the evaluation scores of each algorithm and their standard deviations to pass through autorank. Autorank (and in particular, the Frequentist approach) determines if the data are normal, the populations are homogenous (equal variances) and the number of populations and selects the appropriate statistical test, effect size and methods for determining the confidence interval of the central tendency from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical test chosen by Autorank for the Arrhythmia analysis is the Friedman test. The Friedman test calculates the ranks of the algorithms in each cross-validation fold and then calculates the mean rank of the classifiers over the whole sample. From this it generates the test statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                   meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "Bagging                 2.2  0.720849  0.071371  0.647502  0.794196   \n",
      "Gradient Boosting       2.4  0.720234  0.054841  0.663874  0.776594   \n",
      "Random Forest           2.6  0.729677  0.119578  0.606788  0.852566   \n",
      "AdaBoost                3.8  0.569198  0.178561  0.385693  0.752704   \n",
      "Decision Tree           4.0  0.640950  0.089287   0.54919  0.732709   \n",
      "\n",
      "                  effect_size   magnitude  \n",
      "Bagging                     0  negligible  \n",
      "Gradient Boosting  0.00966815  negligible  \n",
      "Random Forest      -0.0896487  negligible  \n",
      "AdaBoost              1.11529       large  \n",
      "Decision Tree        0.988525       large  \n",
      "pvalue=0.02440590052878717\n",
      "cd=1.9288111473713958\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.9583105444908142, 0.07404874265193939, 0.2385014146566391, 0.9609372019767761, 0.5190461874008179]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=0.005881011843653574\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    }
   ],
   "source": [
    "#Autorank time\n",
    "classifiers = ['Decision Tree', 'Random Forest', 'AdaBoost', 'Bagging', 'Gradient Boosting']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(5):\n",
    "     data[classifiers[i]] = np.random.normal(means[i], sds[i], 10).clip(0, 1)\n",
    "\n",
    "result_frequentist = autorank(data, alpha=0.05, verbose=False, approach='frequentist')\n",
    "print(result_frequentist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the autorank report, the best algorithm for the Arrhythmia Dataset is the Bagging algorithm. A Bagging algorithm functions by splitting the data into multiple training sets upon which a class of learning or optimising methods such as decision trees and neural networks are applied. After training these multiple models on different samples of the same data, the prediction is averaged into a single summary with the reasoning that the averaging of misclassification errors on different data splits gives a better estimate of the predictive ability of a learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caesarian Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the same experimental process to the Caesarian dataset as was done to the Arrhythmia dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 5) (80,)\n"
     ]
    }
   ],
   "source": [
    "#CAESARIAN ANALYSIS\n",
    "#Load Caesarian Dataset\n",
    "df = pd.read_csv('caesarian.csv')\n",
    "#print(df.head())\n",
    "\n",
    "#Split dataset into features and target variable\n",
    "X = df.loc[:, 'age':'heart-problem']\n",
    "y = df.loc[:, 'class']\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 5) (16, 5)\n"
     ]
    }
   ],
   "source": [
    "#Distinguish Training and Test Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = rs)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=10, random_state=1234)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensemble Algorithms\n",
    "\n",
    "#Straight Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state = rs)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(n_estimators = 10, random_state = rs)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#AdaBoost\n",
    "adb = AdaBoostClassifier(DecisionTreeClassifier(random_state = rs), n_estimators = 10, learning_rate=1)\n",
    "adb.fit(X_train, y_train)\n",
    "\n",
    "#Bagging\n",
    "bg = BaggingClassifier(DecisionTreeClassifier(random_state = rs), n_estimators = 10)\n",
    "bg.fit(X_train, y_train)\n",
    "\n",
    "#Gradient Boosting\n",
    "gb = GradientBoostingClassifier(random_state = rs, n_estimators = 10)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on train set: 0.97\n",
      "Decision Tree Accuracy on test set: 0.50\n",
      "\n",
      "Random Forest Accuracy on train set: 0.94\n",
      "Random Forest Accuracy on test set: 0.38\n",
      "\n",
      "AdaBoost Accuracy on train set: 0.97\n",
      "AdaBoost Accuracy on test set: 0.38\n",
      "\n",
      "Bagging Accuracy on train set: 0.94\n",
      "Bagging Accuracy on test set: 0.38\n",
      "\n",
      "Gradient Boosting Accuracy on train set: 0.86\n",
      "Gradient Boosting on test set: 0.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Scores\n",
    "\n",
    "#Standard Decision Tree Score\n",
    "dt_score = dt.score(X_train, y_train)\n",
    "print(f'Decision Tree Accuracy on train set: {dt_score:.2f}')\n",
    "\n",
    "dt_score = dt.score(X_test, y_test)\n",
    "print(f'Decision Tree Accuracy on test set: {dt_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Random Forest Score\n",
    "rf_score = rf.score(X_train, y_train)\n",
    "print(f'Random Forest Accuracy on train set: {rf_score:.2f}')\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print(f'Random Forest Accuracy on test set: {rf_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#AdaBoost Score\n",
    "adb_score = adb.score(X_train, y_train)\n",
    "print(f'AdaBoost Accuracy on train set: {adb_score:.2f}')\n",
    "\n",
    "adb_score = adb.score(X_test, y_test)\n",
    "print(f'AdaBoost Accuracy on test set: {adb_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Bagging Score\n",
    "bg_score = bg.score(X_train, y_train)\n",
    "print(f'Bagging Accuracy on train set: {bg_score:.2f}')\n",
    "\n",
    "bg_score = bg.score(X_test, y_test)\n",
    "print(f'Bagging Accuracy on test set: {bg_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Gradient Boosting\n",
    "gb_score = gb.score(X_train, y_train)\n",
    "print(f'Gradient Boosting Accuracy on train set: {gb_score:.2f}')\n",
    "\n",
    "gb_score = gb.score(X_test, y_test)\n",
    "print(f'Gradient Boosting on test set: {gb_score:.2f}')\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean test accuracy: 0.500\n",
      "Decision Tree Standard Deviation: 0.132\n",
      "\n",
      "Random Forest Mean test accuracy: 0.550\n",
      "Random Forest Standard Deviation: 0.158\n",
      "\n",
      "AdaBoost Mean test accuracy: 0.613\n",
      "AdaBoost Standard Deviation: 0.224\n",
      "\n",
      "Bagging Mean test accuracy: 0.550\n",
      "Bagging Standard Deviation: 0.158\n",
      "\n",
      "Gradient Boosting Mean test accuracy: 0.537\n",
      "Gradient Boosting Standard Deviation: 0.167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "means = []\n",
    "sds = []\n",
    "\n",
    "#Decision Trees\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Tree Mean test accuracy: {np.mean(dt_scores):.3f}\")\n",
    "means += [dt_scores]\n",
    "\n",
    "std = stdev(dt_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(rf_scores)\n",
    "print(f\"Random Forest Mean test accuracy: {np.mean(rf_scores):.3f}\")\n",
    "means += [rf_scores]\n",
    "\n",
    "\n",
    "std = stdev(rf_scores)\n",
    "sds += [std]\n",
    "print(f\"Random Forest Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#AdaBoost\n",
    "adb_scores = cross_val_score(AdaBoostClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(adb_scores)\n",
    "print(f\"AdaBoost Mean test accuracy: {np.mean(adb_scores):.3f}\")\n",
    "means += [adb_scores]\n",
    "\n",
    "std = stdev(adb_scores)\n",
    "sds += [std]\n",
    "print(f\"AdaBoost Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Bagging\n",
    "bg_scores = cross_val_score(BaggingClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(bg_scores)\n",
    "print(f\"Bagging Mean test accuracy: {np.mean(bg_scores):.3f}\")\n",
    "means += [bg_scores]\n",
    "\n",
    "std = stdev(bg_scores)\n",
    "sds += [std]\n",
    "print(f\"Bagging Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Grdient Boosting\n",
    "gb_scores = cross_val_score(GradientBoostingClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(bg_scores)\n",
    "print(f\"Gradient Boosting Mean test accuracy: {np.mean(gb_scores):.3f}\")\n",
    "means += [gb_scores]\n",
    "\n",
    "std = stdev(gb_scores)\n",
    "sds += [std]\n",
    "print(f\"Gradient Boosting Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autorank Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical test chosen by Autorank for the Caesarian analysis is the ANOVA test. The ANOVA test splits and tests observed variance data to gain insight into the relationship between dependent and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                   meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "AdaBoost                2.2  0.654087  0.298911  0.495666  0.812508   \n",
      "Bagging                 2.9  0.581198  0.280014  0.422777  0.739619   \n",
      "Random Forest           3.2  0.528778  0.248934  0.370357  0.687199   \n",
      "Gradient Boosting       3.3  0.535315  0.237468  0.376894  0.693736   \n",
      "Decision Tree           3.4  0.499500  0.157168  0.341079  0.657921   \n",
      "\n",
      "                  effect_size   magnitude  \n",
      "AdaBoost                    0  negligible  \n",
      "Bagging              0.251674       small  \n",
      "Random Forest         0.45557       small  \n",
      "Gradient Boosting    0.439988       small  \n",
      "Decision Tree        0.647351      medium  \n",
      "pvalue=0.4167498968974116\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.7004509568214417, 0.37004631757736206, 0.3947213888168335, 0.5965405702590942, 0.8123753666877747]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.4491624115731875\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janin\\Anaconda3\\lib\\site-packages\\statsmodels\\sandbox\\stats\\multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, ''))\n"
     ]
    }
   ],
   "source": [
    "#Autorank\n",
    "classifiers = ['Decision Tree', 'Random Forest', 'AdaBoost', 'Bagging', 'Gradient Boosting']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(5):\n",
    "     data[classifiers[i]] = np.random.normal(means[i], sds[i], 10).clip(0, 1)\n",
    "\n",
    "result_frequentist = autorank(data, alpha=0.05, verbose=False, approach='frequentist')\n",
    "print(result_frequentist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best algorithm for the Caesarian Dataset according to the mean rank of the Autorank report is the AdaBoost algorithm. Boosting trains each tree on a modified version of the original dataset. AdaBoost does this by assigning higher weights to wrongly classified observations as well as the trained classifiers according to accuracy, while it iteratively trains. This process iterates until complete training data fits without any error.\n",
    "\n",
    "The characteristics of the Caesarian dataset that enabled the AdaBoost algorithm to outperform the others is due to the relatively small size and simplicity of the dataset since due to the nature of the algorithm, Boosting methods would falter for particularly noisy data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website-Phishing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11055, 30) (11055,)\n"
     ]
    }
   ],
   "source": [
    "#Load Website Phishing Dataset\n",
    "df = pd.read_csv('website-phishing.csv')\n",
    "df.columns = df.columns.map(str.strip)\n",
    "X = df.loc[:, 'having_IP_Address':'Statistical_report']\n",
    "y = df.loc[:, 'Class']\n",
    "\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=10, random_state=1234)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensemble Algorithms\n",
    "\n",
    "#Straight Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state = rs)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(n_estimators = 10, random_state = rs)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#AdaBoost\n",
    "adb = AdaBoostClassifier(DecisionTreeClassifier(random_state = rs), n_estimators = 10, learning_rate=1)\n",
    "adb.fit(X_train, y_train)\n",
    "\n",
    "#Bagging\n",
    "bg = BaggingClassifier(DecisionTreeClassifier(random_state = rs), n_estimators = 10)\n",
    "bg.fit(X_train, y_train)\n",
    "\n",
    "#Gradient Boosting\n",
    "gb = GradientBoostingClassifier(random_state = rs, n_estimators = 10)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on train set: 1.00\n",
      "Decision Tree Accuracy on test set: 0.60\n",
      "\n",
      "Random Forest Accuracy on train set: 0.99\n",
      "Random Forest Accuracy on test set: 0.59\n",
      "\n",
      "AdaBoost Accuracy on train set: 1.00\n",
      "AdaBoost Accuracy on test set: 0.63\n",
      "\n",
      "Bagging Accuracy on train set: 0.98\n",
      "Bagging Accuracy on test set: 0.73\n",
      "\n",
      "Gradient Boosting Accuracy on train set: 0.95\n",
      "Gradient Boosting on test set: 0.62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Scores\n",
    "\n",
    "#Standard Decision Tree Score\n",
    "dt_score = dt.score(X_train, y_train)\n",
    "print(f'Decision Tree Accuracy on train set: {dt_score:.2f}')\n",
    "\n",
    "dt_score = dt.score(X_test, y_test)\n",
    "print(f'Decision Tree Accuracy on test set: {dt_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Random Forest Score\n",
    "rf_score = rf.score(X_train, y_train)\n",
    "print(f'Random Forest Accuracy on train set: {rf_score:.2f}')\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print(f'Random Forest Accuracy on test set: {rf_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#AdaBoost Score\n",
    "adb_score = adb.score(X_train, y_train)\n",
    "print(f'AdaBoost Accuracy on train set: {adb_score:.2f}')\n",
    "\n",
    "adb_score = adb.score(X_test, y_test)\n",
    "print(f'AdaBoost Accuracy on test set: {adb_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Bagging Score\n",
    "bg_score = bg.score(X_train, y_train)\n",
    "print(f'Bagging Accuracy on train set: {bg_score:.2f}')\n",
    "\n",
    "bg_score = bg.score(X_test, y_test)\n",
    "print(f'Bagging Accuracy on test set: {bg_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Gradient Boosting\n",
    "gb_score = gb.score(X_train, y_train)\n",
    "print(f'Gradient Boosting Accuracy on train set: {gb_score:.2f}')\n",
    "\n",
    "gb_score = gb.score(X_test, y_test)\n",
    "print(f'Gradient Boosting on test set: {gb_score:.2f}')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean test accuracy: 0.961\n",
      "Decision Tree Standard Deviation: 0.022\n",
      "\n",
      "Random Forest Mean test accuracy: 0.973\n",
      "Random Forest Standard Deviation: 0.011\n",
      "\n",
      "AdaBoost Mean test accuracy: 0.933\n",
      "AdaBoost Standard Deviation: 0.006\n",
      "\n",
      "Bagging Mean test accuracy: 0.969\n",
      "Bagging Standard Deviation: 0.014\n",
      "\n",
      "Gradient Boosting Mean test accuracy: 0.948\n",
      "Gradient Boosting Standard Deviation: 0.007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "means = []\n",
    "sds = []\n",
    "\n",
    "#Decision Trees\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Tree Mean test accuracy: {np.mean(dt_scores):.3f}\")\n",
    "means += [dt_scores]\n",
    "\n",
    "std = stdev(dt_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(rf_scores)\n",
    "print(f\"Random Forest Mean test accuracy: {np.mean(rf_scores):.3f}\")\n",
    "means += [rf_scores]\n",
    "\n",
    "\n",
    "std = stdev(rf_scores)\n",
    "sds += [std]\n",
    "print(f\"Random Forest Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#AdaBoost\n",
    "adb_scores = cross_val_score(AdaBoostClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(adb_scores)\n",
    "print(f\"AdaBoost Mean test accuracy: {np.mean(adb_scores):.3f}\")\n",
    "means += [adb_scores]\n",
    "\n",
    "std = stdev(adb_scores)\n",
    "sds += [std]\n",
    "print(f\"AdaBoost Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Bagging\n",
    "bg_scores = cross_val_score(BaggingClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(bg_scores)\n",
    "print(f\"Bagging Mean test accuracy: {np.mean(bg_scores):.3f}\")\n",
    "means += [bg_scores]\n",
    "\n",
    "std = stdev(bg_scores)\n",
    "sds += [std]\n",
    "print(f\"Bagging Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Grdient Boosting\n",
    "gb_scores = cross_val_score(GradientBoostingClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(bg_scores)\n",
    "print(f\"Gradient Boosting Mean test accuracy: {np.mean(gb_scores):.3f}\")\n",
    "means += [gb_scores]\n",
    "\n",
    "std = stdev(gb_scores)\n",
    "sds += [std]\n",
    "print(f\"Gradient Boosting Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autorank Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                   meanrank    median         mad  ci_lower ci_upper  \\\n",
      "Random Forest          2.45  0.998600  0.00207496  0.809945        1   \n",
      "Gradient Boosting      2.90  0.982654   0.0257176  0.737841        1   \n",
      "Decision Tree          3.00  0.975905   0.0357232  0.847895        1   \n",
      "AdaBoost               3.10  0.917552   0.0982089   0.83122        1   \n",
      "Bagging                3.55  0.957108   0.0635922  0.787674        1   \n",
      "\n",
      "                  effect_size   magnitude  \n",
      "Random Forest               0  negligible  \n",
      "Gradient Boosting    0.874073       large  \n",
      "Decision Tree        0.896957       large  \n",
      "AdaBoost              1.16683       large  \n",
      "Bagging              0.922259       large  \n",
      "pvalue=0.5207728735201385\n",
      "cd=1.9288111473713958\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[0.022618934512138367, 0.0045369029976427555, 0.05054107680916786, 0.013056491501629353, 0.008861812762916088]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.9493909315221907\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n"
     ]
    }
   ],
   "source": [
    "#Autorank time\n",
    "classifiers = ['Decision Tree', 'Random Forest', 'AdaBoost', 'Bagging', 'Gradient Boosting']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(5):\n",
    "     data[classifiers[i]] = np.random.normal(means[i], sds[i], 10).clip(0, 1)\n",
    "\n",
    "result_frequentist = autorank(data, alpha=0.05, verbose=False, approach='frequentist')\n",
    "print(result_frequentist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Website-Phishing Dataset the best method from the Autorank report is the Random Forest algorithm. Random Forest is a Bagging algorithm that combines various decision trees to produce a more generalised model.  Individual decision trees are generated using a random selection of attributes at each node to determine the split and during classification, each tree votes with the most popular class returned.\n",
    "\n",
    "Random forests are more robust to errors and outliers as the generalisation error for a forest converges as long as the number of trees in the forest is large and considers many fewer attributes for each split which makes it efficient for large databases. For this reason, the characteristics of the Website-Phishing dataset that enabled the Random Forest algorithm to outperform the others is due to the vast number of instances and number of attributes present in the multivariate dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Iris Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rs)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=10, random_state=1234)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensemble Algorithms\n",
    "\n",
    "#Straight Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state = rs)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(n_estimators = 10, random_state = rs)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#AdaBoost\n",
    "adb = AdaBoostClassifier(DecisionTreeClassifier(random_state = rs), n_estimators = 10, learning_rate=1)\n",
    "adb.fit(X_train, y_train)\n",
    "\n",
    "#Bagging\n",
    "bg = BaggingClassifier(DecisionTreeClassifier(random_state = rs), n_estimators = 10)\n",
    "bg.fit(X_train, y_train)\n",
    "\n",
    "#Gradient Boosting\n",
    "gb = GradientBoostingClassifier(random_state = rs, n_estimators = 10)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on train set: 1.00\n",
      "Decision Tree Accuracy on test set: 1.00\n",
      "\n",
      "Random Forest Accuracy on train set: 0.98\n",
      "Random Forest Accuracy on test set: 1.00\n",
      "\n",
      "AdaBoost Accuracy on train set: 1.00\n",
      "AdaBoost Accuracy on test set: 1.00\n",
      "\n",
      "Bagging Accuracy on train set: 0.99\n",
      "Bagging Accuracy on test set: 1.00\n",
      "\n",
      "Gradient Boosting Accuracy on train set: 0.99\n",
      "Gradient Boosting on test set: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Scores\n",
    "\n",
    "#Standard Decision Tree Score\n",
    "dt_score = dt.score(X_train, y_train)\n",
    "print(f'Decision Tree Accuracy on train set: {dt_score:.2f}')\n",
    "\n",
    "dt_score = dt.score(X_test, y_test)\n",
    "print(f'Decision Tree Accuracy on test set: {dt_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Random Forest Score\n",
    "rf_score = rf.score(X_train, y_train)\n",
    "print(f'Random Forest Accuracy on train set: {rf_score:.2f}')\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print(f'Random Forest Accuracy on test set: {rf_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#AdaBoost Score\n",
    "adb_score = adb.score(X_train, y_train)\n",
    "print(f'AdaBoost Accuracy on train set: {adb_score:.2f}')\n",
    "\n",
    "adb_score = adb.score(X_test, y_test)\n",
    "print(f'AdaBoost Accuracy on test set: {adb_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Bagging Score\n",
    "bg_score = bg.score(X_train, y_train)\n",
    "print(f'Bagging Accuracy on train set: {bg_score:.2f}')\n",
    "\n",
    "bg_score = bg.score(X_test, y_test)\n",
    "print(f'Bagging Accuracy on test set: {bg_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Gradient Boosting\n",
    "gb_score = gb.score(X_train, y_train)\n",
    "print(f'Gradient Boosting Accuracy on train set: {gb_score:.2f}')\n",
    "\n",
    "gb_score = gb.score(X_test, y_test)\n",
    "print(f'Gradient Boosting on test set: {gb_score:.2f}')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean test accuracy: 0.953\n",
      "Decision Tree Standard Deviation: 0.055\n",
      "\n",
      "Random Forest Mean test accuracy: 0.947\n",
      "Random Forest Standard Deviation: 0.076\n",
      "\n",
      "AdaBoost Mean test accuracy: 0.940\n",
      "AdaBoost Standard Deviation: 0.073\n",
      "\n",
      "Bagging Mean test accuracy: 0.933\n",
      "Bagging Standard Deviation: 0.109\n",
      "\n",
      "Gradient Boosting Mean test accuracy: 0.927\n",
      "Gradient Boosting Standard Deviation: 0.102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "means = []\n",
    "sds = []\n",
    "\n",
    "#Decision Trees\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Tree Mean test accuracy: {np.mean(dt_scores):.3f}\")\n",
    "means += [dt_scores]\n",
    "\n",
    "std = stdev(dt_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(rf_scores)\n",
    "print(f\"Random Forest Mean test accuracy: {np.mean(rf_scores):.3f}\")\n",
    "means += [rf_scores]\n",
    "\n",
    "\n",
    "std = stdev(rf_scores)\n",
    "sds += [std]\n",
    "print(f\"Random Forest Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#AdaBoost\n",
    "adb_scores = cross_val_score(AdaBoostClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(adb_scores)\n",
    "print(f\"AdaBoost Mean test accuracy: {np.mean(adb_scores):.3f}\")\n",
    "means += [adb_scores]\n",
    "\n",
    "std = stdev(adb_scores)\n",
    "sds += [std]\n",
    "print(f\"AdaBoost Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Bagging\n",
    "bg_scores = cross_val_score(BaggingClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(bg_scores)\n",
    "print(f\"Bagging Mean test accuracy: {np.mean(bg_scores):.3f}\")\n",
    "means += [bg_scores]\n",
    "\n",
    "std = stdev(bg_scores)\n",
    "sds += [std]\n",
    "print(f\"Bagging Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Grdient Boosting\n",
    "gb_scores = cross_val_score(GradientBoostingClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(bg_scores)\n",
    "print(f\"Gradient Boosting Mean test accuracy: {np.mean(gb_scores):.3f}\")\n",
    "means += [gb_scores]\n",
    "\n",
    "std = stdev(gb_scores)\n",
    "sds += [std]\n",
    "print(f\"Gradient Boosting Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                   meanrank    median        mad  ci_lower  ci_upper  \\\n",
      "Decision Tree          2.25  0.991074  0.0132332  0.859689         1   \n",
      "Bagging                2.65  0.953300   0.069238  0.701802         1   \n",
      "AdaBoost               2.95  0.922107  0.0613837  0.779098         1   \n",
      "Gradient Boosting      3.45  0.991420  0.0127214  0.720478         1   \n",
      "Random Forest          3.70  0.922444  0.0349123  0.752198  0.969677   \n",
      "\n",
      "                  effect_size   magnitude  \n",
      "Decision Tree               0  negligible  \n",
      "Bagging              0.757846      medium  \n",
      "AdaBoost              1.55325       large  \n",
      "Gradient Boosting  -0.0265957  negligible  \n",
      "Random Forest         2.59958       large  \n",
      "pvalue=0.18697670748815112\n",
      "cd=1.9288111473713958\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[0.001630133599974215, 0.012223594821989536, 0.24799463152885437, 0.01963883824646473, 0.003663484239950776]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.6230137720213405\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n"
     ]
    }
   ],
   "source": [
    "#Autorank time\n",
    "classifiers = ['Decision Tree', 'Random Forest', 'AdaBoost', 'Bagging', 'Gradient Boosting']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(5):\n",
    "     data[classifiers[i]] = np.random.normal(means[i], sds[i], 10).clip(0, 1)\n",
    "\n",
    "result_frequentist = autorank(data, alpha=0.05, verbose=False, approach='frequentist')\n",
    "print(result_frequentist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
